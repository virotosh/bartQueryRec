{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f167992-9ff5-4aaf-b62d-49e7b9da885a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import transformers\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split, RandomSampler, Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from os.path import exists\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "import math\n",
    "import random\n",
    "import re\n",
    "import argparse\n",
    "\n",
    "class LitModel(pl.LightningModule):\n",
    "  # Instantiate the model\n",
    "  def __init__(self, learning_rate, tokenizer, model, hparams):\n",
    "    super().__init__()\n",
    "    self.tokenizer = tokenizer\n",
    "    self.model = model\n",
    "    self.learning_rate = learning_rate\n",
    "    # self.freeze_encoder = freeze_encoder\n",
    "    # self.freeze_embeds_ = freeze_embeds\n",
    "    self.hparams = hparams\n",
    "\n",
    "    if self.hparams.freeze_encoder:\n",
    "      freeze_params(self.model.get_encoder())\n",
    "\n",
    "    if self.hparams.freeze_embeds:\n",
    "      self.freeze_embeds()\n",
    "  \n",
    "  def freeze_embeds(self):\n",
    "    ''' freeze the positional embedding parameters of the model; adapted from finetune.py '''\n",
    "    freeze_params(self.model.model.shared)\n",
    "    for d in [self.model.model.encoder, self.model.model.decoder]:\n",
    "      freeze_params(d.embed_positions)\n",
    "      freeze_params(d.embed_tokens)\n",
    "\n",
    "  # Do a forward pass through the model\n",
    "  def forward(self, input_ids, **kwargs):\n",
    "    return self.model(input_ids, **kwargs)\n",
    "  \n",
    "  def configure_optimizers(self):\n",
    "    optimizer = torch.optim.Adam(self.parameters(), lr = self.learning_rate)\n",
    "    return optimizer\n",
    "\n",
    "  def training_step(self, batch, batch_idx):\n",
    "    # Load the data into variables\n",
    "    src_ids, src_mask = batch[0], batch[1]\n",
    "    tgt_ids = batch[2]\n",
    "    # Shift the decoder tokens right (but NOT the tgt_ids)\n",
    "    decoder_input_ids = shift_tokens_right(tgt_ids, tokenizer.pad_token_id)\n",
    "\n",
    "    # Run the model and get the logits\n",
    "    outputs = self(src_ids, attention_mask=src_mask, decoder_input_ids=decoder_input_ids, use_cache=False)\n",
    "    lm_logits = outputs[0]\n",
    "    # Create the loss function\n",
    "    ce_loss_fct = torch.nn.CrossEntropyLoss(ignore_index=self.tokenizer.pad_token_id)\n",
    "    # Calculate the loss on the un-shifted tokens\n",
    "    loss = ce_loss_fct(lm_logits.view(-1, lm_logits.shape[-1]), tgt_ids.view(-1))\n",
    "\n",
    "    return {'loss':loss}\n",
    "\n",
    "  def validation_step(self, batch, batch_idx):\n",
    "\n",
    "    src_ids, src_mask = batch[0], batch[1]\n",
    "    tgt_ids = batch[2]\n",
    "\n",
    "    decoder_input_ids = shift_tokens_right(tgt_ids, tokenizer.pad_token_id)\n",
    "    \n",
    "    # Run the model and get the logits\n",
    "    outputs = self(src_ids, attention_mask=src_mask, decoder_input_ids=decoder_input_ids, use_cache=False)\n",
    "    lm_logits = outputs[0]\n",
    "\n",
    "    ce_loss_fct = torch.nn.CrossEntropyLoss(ignore_index=self.tokenizer.pad_token_id)\n",
    "    val_loss = ce_loss_fct(lm_logits.view(-1, lm_logits.shape[-1]), tgt_ids.view(-1))\n",
    "\n",
    "    return {'loss': val_loss}\n",
    "  \n",
    "  # Method that generates text using the BartForConditionalGeneration's generate() method\n",
    "  def generate_text(self, text, eval_beams, early_stopping = False, max_len = 500):\n",
    "    ''' Function to generate text '''\n",
    "    generated_ids = self.model.generate(\n",
    "        text[\"input_ids\"],\n",
    "        attention_mask=text[\"attention_mask\"],\n",
    "        use_cache=True,\n",
    "        decoder_start_token_id = self.tokenizer.pad_token_id,\n",
    "        num_beams= eval_beams,\n",
    "        max_length = max_len,\n",
    "        min_length = 50,\n",
    "        early_stopping = early_stopping\n",
    "    )\n",
    "    return [self.tokenizer.decode(w, skip_special_tokens=True, clean_up_tokenization_spaces=True) for w in generated_ids]\n",
    "\n",
    "    def freeze_params(model):\n",
    "      ''' Function that takes a model as input (or part of a model) and freezes the layers for faster training\n",
    "          adapted from finetune.py '''\n",
    "      for layer in model.parameters():\n",
    "        layer.requires_grade = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc44976-85a1-402e-b52e-1ccd55a39e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataloading module as per the PyTorch Lightning Docs\n",
    "class SummaryDataModule(pl.LightningDataModule):\n",
    "  def __init__(self, tokenizer, data_file, batch_size, num_examples = 20000):\n",
    "    super().__init__()\n",
    "    self.tokenizer = tokenizer\n",
    "    self.data_file = data_file\n",
    "    self.batch_size = batch_size\n",
    "    self.num_examples = num_examples\n",
    "  \n",
    "  # Loads and splits the data into training, validation and test sets with a 60/20/20 split\n",
    "  def prepare_data(self):\n",
    "    self.data = pd.read_csv(self.data_file)[:self.num_examples]\n",
    "    self.train, self.validate, self.test = np.split(self.data.sample(frac=1), [int(.6*len(self.data)), int(.8*len(self.data))])\n",
    "\n",
    "  # encode the sentences using the tokenizer  \n",
    "  def setup(self, stage):\n",
    "    self.train = encode_sentences(self.tokenizer, self.train['source'], self.train['target'])\n",
    "    self.validate = encode_sentences(self.tokenizer, self.validate['source'], self.validate['target'])\n",
    "    self.test = encode_sentences(self.tokenizer, self.test['source'], self.test['target'])\n",
    "\n",
    "  # Load the training, validation and test sets in Pytorch Dataset objects\n",
    "  def train_dataloader(self):\n",
    "    dataset = TensorDataset(self.train['input_ids'], self.train['attention_mask'], self.train['labels'])                          \n",
    "    train_data = DataLoader(dataset, sampler = RandomSampler(dataset), batch_size = self.batch_size)\n",
    "    return train_data\n",
    "\n",
    "  def val_dataloader(self):\n",
    "    dataset = TensorDataset(self.validate['input_ids'], self.validate['attention_mask'], self.validate['labels']) \n",
    "    val_data = DataLoader(dataset, batch_size = self.batch_size)                       \n",
    "    return val_data\n",
    "\n",
    "  def test_dataloader(self):\n",
    "    dataset = TensorDataset(self.test['input_ids'], self.test['attention_mask'], self.test['labels']) \n",
    "    test_data = DataLoader(dataset, batch_size = self.batch_size)                   \n",
    "    return test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cea1eb-9070-4e0a-8dcd-d5aaaf4cfb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hparams to pass in the model\n",
    "hparams = argparse.Namespace()\n",
    "\n",
    "hparams.freeze_encoder = True\n",
    "hparams.freeze_embeds = True\n",
    "hparams.eval_beams = 4\n",
    "\n",
    "\n",
    "def shift_tokens_right(input_ids, pad_token_id):\n",
    "  \"\"\" Shift input ids one token to the right, and wrap the last non pad token (usually <eos>).\n",
    "      This is taken directly from modeling_bart.py\n",
    "  \"\"\"\n",
    "  prev_output_tokens = input_ids.clone()\n",
    "  index_of_eos = (input_ids.ne(pad_token_id).sum(dim=1) - 1).unsqueeze(-1)\n",
    "  prev_output_tokens[:, 0] = input_ids.gather(1, index_of_eos).squeeze()\n",
    "  prev_output_tokens[:, 1:] = input_ids[:, :-1]\n",
    "  return prev_output_tokens\n",
    "\n",
    "def encode_sentences(tokenizer, source_sentences, target_sentences, max_length=300, pad_to_max_length=True, return_tensors=\"pt\"):\n",
    "  ''' Function that tokenizes a sentence \n",
    "      Args: tokenizer - the BART tokenizer; source and target sentences are the source and target sentences\n",
    "      Returns: Dictionary with keys: input_ids, attention_mask, target_ids\n",
    "  '''\n",
    "\n",
    "  input_ids = []\n",
    "  attention_masks = []\n",
    "  target_ids = []\n",
    "  tokenized_sentences = {}\n",
    "\n",
    "  for sentence in source_sentences:\n",
    "    encoded_dict = tokenizer(\n",
    "          sentence.lower(),\n",
    "          max_length=max_length,\n",
    "          padding=\"max_length\" if pad_to_max_length else None,\n",
    "          truncation=True,\n",
    "          return_tensors=return_tensors,\n",
    "          add_prefix_space = True\n",
    "      )\n",
    "\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "  input_ids = torch.cat(input_ids, dim = 0)\n",
    "  attention_masks = torch.cat(attention_masks, dim = 0)\n",
    "\n",
    "  for sentence in target_sentences:\n",
    "    encoded_dict = tokenizer(\n",
    "          sentence.lower(),\n",
    "          max_length=max_length,\n",
    "          padding=\"max_length\" if pad_to_max_length else None,\n",
    "          truncation=True,\n",
    "          return_tensors=return_tensors,\n",
    "          add_prefix_space = True\n",
    "      )\n",
    "    # Shift the target ids to the right\n",
    "    # shifted_target_ids = shift_tokens_right(encoded_dict['input_ids'], tokenizer.pad_token_id)\n",
    "    target_ids.append(encoded_dict['input_ids'])\n",
    "\n",
    "  target_ids = torch.cat(target_ids, dim = 0)\n",
    "  \n",
    "\n",
    "  batch = {\n",
    "      \"input_ids\": input_ids,\n",
    "      \"attention_mask\": attention_masks,\n",
    "      \"labels\": target_ids,\n",
    "  }\n",
    "\n",
    "  return batch\n",
    "\n",
    "\n",
    "# Load the model\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration, AdamW, BartConfig\n",
    "\n",
    "# Load pre-trained\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-base', add_prefix_space=False, bos_token=\"<s>\", eos_token=\"</s>\")\n",
    "bart_model = BartForConditionalGeneration.from_pretrained(\n",
    "    \"facebook/bart-base\")\n",
    "\n",
    "def find_best_epoch(ckpt_folder):\n",
    "    \"\"\"\n",
    "    Find the highest epoch in the Test Tube file structure.\n",
    "    :param ckpt_folder: dir where the checpoints are being saved.\n",
    "    :return: Integer of the highest epoch reached by the checkpoints.\n",
    "    \"\"\"\n",
    "    ckpt_files = os.listdir(ckpt_folder)  # list of strings\n",
    "    epochs = [int(filename.split('step=')[-1].split('.')[0]) for filename in ckpt_files]  # 'epoch={int}.ckpt' filename format\n",
    "    best_epoch = max(epochs)\n",
    "    for filename in ckpt_files:\n",
    "        if str('{}.ckpt'.format(best_epoch)) in filename:\n",
    "            return filename\n",
    "    return best_epoch\n",
    "\n",
    "def generate_queries(seed_query, num_queries, model_, noise_percent = 0.25, multiple_queries = False, max_query_history = 3):\n",
    "  ''' Function that generates queries based on previously generated queries \n",
    "      Args: seed_query - a prior query\n",
    "            num_queries - number of queries to generate\n",
    "            model_ - model used to generate\n",
    "            multiple_queries - model generates based on prior queries\n",
    "            max_query_history - maximum number of prior queries\n",
    "      Returns a list with num_queries\n",
    "  '''\n",
    "  # Put the model on eval mode\n",
    "  model_.to(torch.device('cpu'))\n",
    "  model_.eval()\n",
    "  queries = []\n",
    "  queries.append(seed_query)\n",
    "  prompt_tokens = tokenizer(seed_query.lower(), max_length = 300, return_tensors = \"pt\", truncation = True)\n",
    "  print(prompt_tokens)\n",
    "  query = model_.generate_text(prompt_tokens, eval_beams = 4)\n",
    "  print('pred',query[0].strip())\n",
    "  return query[0].strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef729c7-ebf1-4873-9950-35c9818971be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    torch.cuda.empty_cache()\n",
    "    suggestions = {}\n",
    "    # each file is a user, 2 columns: source, target \n",
    "    # \"source\" column is prior query, \"target\" column is predicting target\n",
    "    # each row represents a query\n",
    "    for filename in os.listdir(\"./mytestdata/full\"):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            user = filename.replace('.csv','')\n",
    "            suggestions[user] = []\n",
    "            # split data to train / test\n",
    "            ratio = .7\n",
    "            print('--------------',user,ratio,'----------')\n",
    "            allindex = pd.read_csv('./mytestdata/full/'+filename).index.values\n",
    "            splitindex = allindex[int(len(allindex)*ratio)]\n",
    "            pred_index = allindex[int(len(allindex)*ratio):]\n",
    "            \n",
    "            # Load the data into the model for training\n",
    "            summary_data = SummaryDataModule(tokenizer, './mytestdata/full/'+filename,\n",
    "                                             batch_size = 14, num_examples = splitindex)\n",
    "\n",
    "\n",
    "            model = LitModel(learning_rate = 2e-5, tokenizer = tokenizer, model = bart_model, hparams = hparams)\n",
    "\n",
    "\n",
    "            ckpt_dir = './checkpoint_files_2/'+user+'_full'\n",
    "            checkpoint = ModelCheckpoint(ckpt_dir)\n",
    "            # Load the model from saved checkpoint if exists\n",
    "            if exists(ckpt_dir):\n",
    "                best_epoch = find_best_epoch(ckpt_dir)\n",
    "                print(ckpt_dir+'/'+best_epoch)\n",
    "                trainer = pl.Trainer(gpus = 4,\n",
    "                                 max_epochs = 20,\n",
    "                                 min_epochs = 20,\n",
    "                                 auto_lr_find = False,\n",
    "                                 resume_from_checkpoint = ckpt_dir+'/'+best_epoch,\n",
    "                                 progress_bar_refresh_rate = 10)\n",
    "            # finetune from stratch\n",
    "            else:\n",
    "                trainer = pl.Trainer(gpus = 4,\n",
    "                                 max_epochs = 20,\n",
    "                                 min_epochs = 20,\n",
    "                                 auto_lr_find = False,\n",
    "                                 checkpoint_callback = checkpoint,\n",
    "                                 progress_bar_refresh_rate = 10)\n",
    "\n",
    "            # Fit the instantiated model to the data\n",
    "            trainer.fit(model, summary_data)\n",
    "\n",
    "            pred_df = pd.read_csv('./mytestdata/full/'+filename)[splitindex:]\n",
    "            for index, row in pred_df.iterrows():\n",
    "                pred_target = generate_queries(seed_query = row['source'], num_queries = 2, model_ = model,\n",
    "                                       noise_percent = 0.25, multiple_queries = True, max_query_history = 2)\n",
    "                suggestions[user].append([row['target'],pred_target,row['source'],index])\n",
    "\n",
    "    with open('./suggestions.json', 'w') as outfile:\n",
    "        json.dump(suggestions, outfile)\n",
    "\n",
    "main()  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
